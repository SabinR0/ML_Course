{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Laboratory 02: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "Students should understand and be able use a logistic regression model in Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical aspects\n",
    "\n",
    "Logistic regression is a widely used model for estimating **probabilities** (e.g. values between 0 and 1). It is also widely used for **binary classification**.\n",
    "\n",
    "### Logistic regression: the model\n",
    "\n",
    "We have an input vector $x$ and a predicted value $y$:\n",
    "$$X = \\begin{bmatrix} x_1 & x_2 & \\dots & x_N \\end{bmatrix} \\rightarrow y$$\n",
    "\n",
    "The logistic regression model: the output is assumed to be a the **sigmoid function**  applied to a **linear combination** of the inputs.\n",
    "The sigmoid function is also known as the **logistic function**, hence the name.\n",
    "$$y \\approx \\frac{1}{1 + e^{- w_1 x_1 - w_2 x_2 - ... - w_N x_N - b}}.$$\n",
    "\n",
    "This can be understood as a **sequence of two steps**:\n",
    "\n",
    "1. Compute a **linear combination $z$** of the inputs:\n",
    "    $$z = w_1 x_1 + w_2 x_2 + ... + w_N x_N + b$$\n",
    "    \n",
    "2. Pass $z$ through the **sigmoid** function $\\sigma(z)$:\n",
    "    $$y = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "\n",
    "![Logistic regression as 1 neuron](img/Neuron2.png)  \n",
    "\n",
    "Both steps can be represented asn one \"neuron\" like this:\n",
    "\n",
    "![Logistic regression as 1 neuron](img/Neuron.png)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid function\n",
    "\n",
    "Let's take a look at the sigmoid function $\\sigma{z}$:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/Sigmoid.svg\" align=\"center\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Notes**: \n",
    "\n",
    "- The output value is always between 0 and 1. This makes the result good for modeling **probabilities**, but not good for other types of numeric values.\n",
    "- The sigmoid can be understood as follows:\n",
    "   - when $z$ is much bigger than 0, the result is practically 1\n",
    "   - when $z$ is much smaller than 0, the result is practically 0\n",
    "   - when $z$ is around 0, there is a \"transition zone\" from 0 to 1. In particular, when $z=0$, the output is right at the middle, $\\sigma(0) = 0.5$.\n",
    "   \n",
    "This makes it similar to classification: if $z$ much larger than 0, data belongs to class 1; if $z$ much smaller than 0, data belongs to class 0; $z=0$ is the frontier. Around the frontier, we have less confidence in the classification (a sort of \"gray area\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: the parameters\n",
    "\n",
    "The parameters of the logisticr regression model are the **weights** $w_1, w_2, ... w_N$ and the **bias** value $b$ (also known as the **intercept**). This is similar to the linear regression.\n",
    "\n",
    "In a similar way to linear regression, we can consider $b$ as just another weight $w_i$ which multiplies a constant input of 1.\n",
    "In this way, we can compute $z$ as the inner product of two vectors:\n",
    "\n",
    "$$\\begin{bmatrix} x_1 & x_2 & \\dots & w_N & b \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\\\ b \\end{bmatrix} = z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function (loss function)\n",
    "\n",
    "Any cost function can be chose (the cost function can always be chosen independent of the model). \n",
    "However, because the outputs are typically understood as **probabilities**, we should use a distance function which is appropriate to probabilities: the cross-entropy (also known as the Kullback-Leibler distance).\n",
    "\n",
    "The **cross-entropy** loss function (cost function) for a single value $y$:\n",
    "\n",
    "$$L(y, \\hat{y}) = - y \\log{\\hat{y}} - (1-y) \\log{(1 - \\hat{y})}$$\n",
    "\n",
    "Usually the true value $y$ is either 0 or 1 (e.g. in a  classification: 0 = cat, 1 = dog). In this case the cross-entropy should be understood as:\n",
    "\n",
    "$$L(y, \\hat{y}) = \n",
    "\\begin{cases}\n",
    "- \\log(\\hat{y})     = \\log{\\frac{1}{\\hat{y}}}, & \\textrm{ when true output is } y = 1; \\\\\n",
    "- \\log(1 - \\hat{y}) = \\log{\\frac{1}{1-\\hat{y}}}, & \\textrm{ when true output is } y = 0;\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In both cases, the cost is 0 if $\\hat{y} = y$, and it is $\\infty$ when $\\hat{y}$ is exactly the opposite, as depicted below:\n",
    "\n",
    "![Cross-entropy loss when true class $y$ is either 0 or 1](img/CrossEntropy.png)\n",
    "\n",
    "When there are N input-output pairs, the overall cost is the average for all:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_i L(y^i, \\hat{y}^i) = \\frac{1}{N} \\sum_i \\left(- y \\log{\\hat{y}} - (1-y) \\log{1 - \\hat{y}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train logistic regression?\n",
    "\n",
    "**Training** = **learning** = finding good values for the unknown parameters.\n",
    "\n",
    "There is no closed-form solution. The solution is found using numerical algorithms.\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "We can use the same Gradient Descent approach to train the parameters of the model.\n",
    "\n",
    "- Initialize parameter vector $W$ with random values\n",
    "- Repeat:\n",
    "  - Compute cost $J$ (forward pass)\n",
    "  - Compute the gradient with vector of $J$ with respect to parameters $W$, $\\frac{dJ}{dW}$\n",
    "  - Update parameters: $W = W - \\mu \\frac{dJ}{dW}$\n",
    "\n",
    "For the logistic regresion with the cross-entropy loss, the gradient is equal to:\n",
    "\n",
    "$$\\frac{dJ}{dW} = X^* (\\hat{Y} - Y).$$\n",
    "\n",
    "(TODO: prove this).\n",
    "\n",
    "That's right, the gradient of logistic regression with cross-entropy loss is the same as the gradient of linear regression with quadratic loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of logistic regression in 2D\n",
    "\n",
    "Below is an example of logistic regression in 2D (there are 2 inputs $[x_1, x_2]$), trained with Gradient Descent.\n",
    "\n",
    "Since we have two inputs, there will be 3 parameters in the $W$ vector (including $b$).\n",
    "\n",
    "**Note**: for a nice graphical animation, please run the code below **directly in Matlab** (file `L2_LogisticRegression_Visualize2D.m`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear all\n",
    "close all\n",
    "\n",
    "% Load some data\n",
    "load('LogisticReg.mat');  % the inputs are X, the outputs are Y\n",
    "\n",
    "% Extend X with a column of 1\n",
    "N = size(X,1);\n",
    "X = [X, ones(N,1)];\n",
    "\n",
    "% Initialize the three parameters in W\n",
    "W = randn(3,1);\n",
    "\n",
    "% Repeat Gradient Descent iterations\n",
    "for iter=1:1000\n",
    "    % Predict\n",
    "    z = X * W;\n",
    "    y_pred = 1./(1 + exp(-z));\n",
    "\n",
    "    % Cost function\n",
    "    J(iter) = 1/N * sum(-Y .* log(y_pred) - (1-Y).* log(1-y_pred));\n",
    "\n",
    "    % Gradient (derivatives)\n",
    "    dW = X' * (y_pred - Y);\n",
    "\n",
    "    % Update\n",
    "    mu = 0.0001;\n",
    "    W = W - mu*dW;\n",
    "    \n",
    "    %===================================\n",
    "    % Plotting stuff\n",
    "    %===================================\n",
    "    \n",
    "    % Plot decision boundary\n",
    "    subplot(1,2,1)\n",
    "    gscatter(X(:,1),X(:,2),Y)\n",
    "    title('Data plot');\n",
    "    hold on\n",
    "    \n",
    "    % Plot decision line on top of points\n",
    "    xx = linspace(-2, 3, 1000);\n",
    "    yy = -W(1)/W(2) * xx - W(3)/W(2);\n",
    "    hold on\n",
    "    plot(xx, yy, 'LineWidth',2);\n",
    "    legend('Class 0', 'Class 1', 'Boundary between classes (output = 0.5)');\n",
    "    hold off\n",
    "    axis([-2, 3, -2, 3])\n",
    "    axis square     \n",
    "    \n",
    "    % On right side, plot grayscale regions\n",
    "    subplot(1,2,2)\n",
    "    I = zeros(500,500);\n",
    "    x_values = linspace(-2, 3, 500);\n",
    "    y_values = linspace(-2, 3, 500);\n",
    "    for i = 1:length(x_values)\n",
    "        x = x_values(i);\n",
    "        for j = 1:length(y_values)\n",
    "            y = y_values(j);\n",
    "            % Compute prediction in point (i,j)\n",
    "            z = [x, y, 1] * W;\n",
    "            I(501-i,j) = 1 / (1 + exp(-z));   % 0 = black,  1 = white, in-between = gray\n",
    "        end\n",
    "    end\n",
    "    I = fliplr(flipud(I'));\n",
    "    imshow(I);\n",
    "    title('Sigmoid output');\n",
    "    hold on\n",
    "    \n",
    "    % Plot line on right side as well\n",
    "    subplot(1,2,2)\n",
    "    xx_rescaled = (xx + 2)*500/5+1;\n",
    "    yy_rescaled = 500 - (yy + 2)*500/5+1;\n",
    "    %plot(xx_rescaled, yy_rescaled, 'LineWidth',2) \n",
    "    plot(xx_rescaled, yy_rescaled, 'LineWidth',2) \n",
    "    hold off\n",
    "    %axis([-2, 3, -2, 3])\n",
    "    axis square     \n",
    "    \n",
    "    drawnow()\n",
    "   \n",
    "    %pause(0.1)\n",
    "    %===================================\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "\n",
    "![Logistic regression attempts to draw one linear frontier between the classes](L2_LogisticRegression_Visualize2D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression = One separating line\n",
    "\n",
    "The take-home message from the above example is:\n",
    "\n",
    "**Logistic regression draws one linear frontier (a \"hyperplane\") in the classification space.**\n",
    "\n",
    "In the following episode we will see how we can combine multiple neurons (multiple hyperplanes) into forming any classification boundary, however complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matlab function doing the job for us\n",
    "\n",
    "Linear regression can be fitted in Matlab using the function `fitglm()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mdl = \n",
      "\n",
      "\n",
      "Generalized linear regression model:\n",
      "    logit(y) ~ 1 + x1 + x2\n",
      "    Distribution = Binomial\n",
      "\n",
      "Estimated Coefficients:\n",
      "                   Estimate      SE        tStat       pValue  \n",
      "                   ________    _______    _______    __________\n",
      "\n",
      "    (Intercept)    -4.7378     0.50621    -9.3595     8.015e-21\n",
      "    x1              4.8554     0.53635     9.0526    1.3957e-19\n",
      "    x2              4.6079     0.53354     8.6366    5.7933e-18\n",
      "\n",
      "\n",
      "600 observations, 597 error degrees of freedom\n",
      "Dispersion: 1\n",
      "Chi^2-statistic vs. constant model: 637, p-value = 5.55e-139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = X(:,1:2);   % Keep only the original two components, not the ones we added extra\n",
    "mdl = fitglm(X, Y, 'Distribution', 'binomial') % X are the inputs, Y is the target vector, mdl is a model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sime predictions with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ans =\n",
      "\n",
      "    0.0087\n",
      "\n",
      "\n",
      "ans =\n",
      "\n",
      "    0.0806\n",
      "\n",
      "\n",
      "ans =\n",
      "\n",
      "    0.9486\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Predict \n",
    "mdl.predict([0,0]) \n",
    "mdl.predict([0,0.5]) \n",
    "mdl.predict([3,-1.5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which class do the previous inputs belong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practical work\n",
    "\n",
    "We use the same data as in linear regression, but instead we try to predict if one of the two possibilities: the quality score is <=5 (class 0) or the quality score is > 5 (class 1).\n",
    "\n",
    "As a reminder, the data comes from here: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009, and contains 11 numerical chemical measurements for some different brands of red wines, together with a quality score indicated by buyers (quality goes from 3 to 8).\n",
    "\n",
    "Inputs:\n",
    "\n",
    "   - 1 - fixed acidity\n",
    "   - 2 - volatile acidity\n",
    "   - 3 - citric acid\n",
    "   - 4 - residual sugar\n",
    "   - 5 - chlorides\n",
    "   - 6 - free sulfur dioxide\n",
    "   - 7 - total sulfur dioxide\n",
    "   - 8 - density\n",
    "   - 9 - pH\n",
    "   - 10 - sulphates\n",
    "   - 11 - alcohol \n",
    "   \n",
    "Outputs:\n",
    "\n",
    "   - 12 - quality\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Data = readmatrix('winequality-red.csv');\n",
    "X = Data(:,1:11);       % 11 columns for the inputs\n",
    "N = size(Data,1);       % The number of wines in the set (1599)\n",
    "\n",
    "Y = Data(:,12) > 5;     % make 1 column for the output: 1 if score > 5, 0 if score <= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the X matrix so we can treat the bias $b$ as just another weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = [X ones(N,1)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the weights to some random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W =\n",
      "\n",
      "    2.7694\n",
      "   -1.3499\n",
      "    3.0349\n",
      "    0.7254\n",
      "   -0.0631\n",
      "    0.7147\n",
      "   -0.2050\n",
      "   -0.1241\n",
      "    1.4897\n",
      "    1.4090\n",
      "    1.4172\n",
      "    0.6715\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = randn(12, 1)   % a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Compute and show the cost function with the above weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Implement optimization with Gradient Descent\n",
    "\n",
    "You can implement a visualizaiton just like in the example provided, by copying and adapting the code.\n",
    "You cannot plot all the 11 dimensions of the input data, so pick only two of them to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% To fill in\n",
    "%======================\n",
    "\n",
    "W = randn(12, 1);           % initialize parameters randomly\n",
    "\n",
    "number_of_epochs = 1000;    % set number of iterations\n",
    "\n",
    "for iter = 1:number_of_epochs\n",
    "    \n",
    "    % Compute predictions:\n",
    "    Y-pred = ...\n",
    "    \n",
    "    % Compute cost:\n",
    "    J(iter) = 1/N * ...\n",
    "    \n",
    "    % Compute derivatives according to the given formula\n",
    "    dW = ...\n",
    "    \n",
    "    % Update the weights\n",
    "    mu = 0.0001;           % try multiple values here\n",
    "    W = W - mu * dW;\n",
    "    \n",
    "    % Store the weights history\n",
    "    W_hist(:,i) = W;\n",
    "end\n",
    "\n",
    "% Plot the error and the evolution of the weights\n",
    "plot(J)\n",
    "figure\n",
    "plot(W_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Compute the solution with the Matlab function `fitglm()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final questions\n",
    "\n",
    "1. In our example, the parameters $W$ keep updating forever, making the gray transition area smaller and smaller, but the actual frontier does not change much. Why does this happen? How can we prevent it?\n",
    "\n",
    "2. What happens if the two classes are **unbalanced** (many more inputs in one class compared to the other)?\n",
    "\n",
    "2. Suggest some good termination conditions for Gradient Descent (i.e. when should we stop the iterations)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
