{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Laboratory 03: MultiLayer Perceptron networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "Students should understand and be able use a multi-layer fully-connected networks in Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical aspects\n",
    "\n",
    "Multi-layer perceptron (fully-connected) neural networks are widely used for classificaton of small, simple datasets.\n",
    "\n",
    "### Multilayer perceptron\n",
    "\n",
    "#### Cascading neurons \n",
    "\n",
    "We start from last week's essential message: **A single neuron creates a single hyperplane and separates the input space in two categories 0 or 1)**\n",
    "  - **\"neuron\"** = one logistic regression operation\n",
    "  - **\"hyperplane\"** = a linear boundary surface, with dimension N-1 \n",
    "  - with a smooth sigmoid transition zone between the two classes\n",
    "  \n",
    "What if we have a dataset as follows? How to do classification here?\n",
    "\n",
    "<div>\n",
    "<img src=img/DatasetAngled1.png align=\"center\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "![How to separate the classes in this dataset? ]()\n",
    "\n",
    "\n",
    "Solution: use **two neurons**: \n",
    "  \n",
    "  - each one draws a hyperplane (a line)\n",
    "  - aggregate their results into the final outcome: \"When both neurons say 1, output class is 1. Otherwise, output class is 0\".\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=img/DatasetAngled2.png align=\"center\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Combining the results of both neurons in the final result is **also done with a (third) neuron**. Thus, we have **cascading neurons**.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=img/Network2plus1.png align=\"center\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Neurons operating on the same inputs form a **layer**. We have two layers now:\n",
    "\n",
    "- The inputs (this does not contain neurons, just the inputs, but it is commonly named \"the input layer\")\n",
    "- The hidden layer (middle)\n",
    "- The output layer (the output neuron)\n",
    "\n",
    "What if we want a boundary composed of 3 sides? Use three neurons in the hidden layer.\n",
    "\n",
    "What if we want a curved boundary? Use many more neurons (approximate the curve from many lines)\n",
    "\n",
    "**Any hypersurface** can be obtained with just two layers, provided there are enough neurons in the hidden layer:\n",
    "\n",
    "  1. The hidden layer draws some hyperplanes (e.g. lines)\n",
    "  2. The output layer combines the results into output values\n",
    "\n",
    "#### Multiple outputs\n",
    "\n",
    "What if we have 4 output classes?\n",
    "\n",
    "Have 4 neurons in the output layer, one for each class. When the input belongs to class $k$, the $k$-th neuron should produce 1, and all the others should produce 0.\n",
    "\n",
    "**One-hot encoding**: When we train the network, we need to tell it what is the desired output (target). This is known as **encoding**. \n",
    "For an input of class $k$, we tell the network to produce a vector with a single value of 1, on position $k$.\n",
    "$$\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{bmatrix}$$\n",
    "\n",
    "After training, when running the model, we look at **the location** of the highest value and the location is the predicted class.\n",
    "\n",
    "#### Multiple layers\n",
    "\n",
    "We can actually have more than 2 layers in a network. We can have as many as we want! Interpretation:\n",
    "\n",
    "   - first hidden layer draws some hyperplanes\n",
    "   - next layer combines hyperplanes into some simpler shapes\n",
    "   - next layer combines the simple shapes into more complex shapes\n",
    "   - ....\n",
    "   - final layer gives the output\n",
    "\n",
    "In practice, it is often better to have **more layers with fewer neurons** than 2 layers but with a huge hidden layer.\n",
    "\n",
    "However, training many layers and many neurons is **difficult**, i.e. it can overfit, become unstable, etc.\n",
    "\n",
    "#### Matrix form\n",
    "\n",
    "**One neuron** does a linear combination of the inputs, followed by activation function:\n",
    "$$\\begin{bmatrix} w_1 & x_w & \\dots & w_N & b \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1 \\end{bmatrix} = z \n",
    "\\rightarrow a$$\n",
    "\n",
    "**A layer of $M$ neurons is just $M$ neurons next to each other**:\n",
    "$$\\begin{bmatrix} \n",
    "w_{11} & w_{12} & \\dots & w_{1N} & b_1 \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2N} & b_2 \\\\\n",
    "\\vdots  & \\vdots & \\dots & \\vdots & \\vdots \\\\\n",
    "w_{M1} & w_{M2} & \\dots & w_{MN} & b_M \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "z_1 \\\\ z_2 \\\\ \\dots \\\\ z_M\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \n",
    "\\begin{bmatrix} \n",
    "a_1 \\\\ a_2 \\\\ \\dots \\\\ a_M\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each layer is characterized by the weight matrix $W$.\n",
    "\n",
    "The whole network can be understood as a sequence of matrix multiplications and activation functions.\n",
    "\n",
    "**The next layer** takes as inputs the outputs $a_i$ of the previos layer, and does the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The multi-layer perceptron model contains $L$ layers, each layer consisting of a matrix multiplication and activation function:\n",
    "\n",
    "$$\\begin{align}z^{1} =& W^{1} \\cdot X \\\\\n",
    "a^{1} =& activation(z^{1}) \\\\\n",
    "\\\\\n",
    "z^{2} =& W^{2} \\cdot a^{1} \\\\\n",
    "a^{2} =& activation(z^{2}) \\\\\n",
    "\\\\ \n",
    "... \\\\\n",
    "z^{k} =& W^{k} \\cdot a^{k-1} \\\\\n",
    "a^{k} =& activation(z^{k})\\end{align}$$\n",
    "\n",
    "Here, $W^{k}$ is a matrix and $z{k}$, $a{k}$ are vectors (columns).\n",
    "\n",
    "The activation function can be the **sigmoid**, **ReLU**, **tanh** etc. Typically the output used sigmoid, but all others are up to the designer.\n",
    "\n",
    "**Inputs**: \n",
    "\n",
    "   - a matrix $X$ with every input vector being a column (according to the equations below; we can also transpose all matrices and vectors, \n",
    "   if we want).\n",
    " \n",
    "**Outputs** (assuming one-hot encoding):\n",
    " \n",
    "  - a vector $\\hat{y}$ which should be understood as scores/probability of belonging in each class\n",
    "  - the **location of the maximum** value gives the predicted class\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model parameters\n",
    "\n",
    "The model parameters are the weight matrices $W^{k}$ of every layer. The element $w^{k}_{ij}$ is the weight in the $k$-th layer, $i$-th neuron, $j$-th input of it.\n",
    "\n",
    "Every neuron has a bias input. We presume that the bias is included in the weight matrices,\n",
    "like we did until now (e.g. like a fake input equal to 1 is appended to the input of every layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost function\n",
    "\n",
    "For classification, the cross-entropy is used.\n",
    "\n",
    "For a single input:\n",
    "\n",
    "$$L(y, \\hat{y}) = - y_1 \\log{\\hat{y_1}} - \\dots - y_n \\log{\\hat{y_n}} = -\\log{\\hat{y_{class}}},$$\n",
    "\n",
    "where $\\hat{y_{class}}$ is the model's predicted probability for the true class of the input.\n",
    "\n",
    "For multiple inputs: do the average of all\n",
    "$$J = \\frac{1}{N} \\sum_i L(y^i, \\hat{y}^i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is done with **backpropagation** and gradient descent (or some variant of it).\n",
    "\n",
    "**Backpropagation** = the technique to compute the derivatives of $J$  with respect to all parameters in the network.\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "Assume we have a network with 4 layers.\n",
    "\n",
    "$$\\begin{align}z^{1} =& W^{1} \\cdot X \\\\\n",
    "a^{1} =& activation(z^{1}) \\\\\n",
    "\\\\\n",
    "z^{2} =& W^{2} \\cdot a^{1} \\\\\n",
    "a^{2} =& activation(z^{2}) \\\\\n",
    "\\\\ \n",
    "z^{3} =& W^{3} \\cdot a^{2} \\\\\n",
    "a^{3} =& activation(z^{3}) \\\\\n",
    "\\\\ \n",
    "z^{4} =& W^{4} \\cdot a^{3} \\\\\n",
    "a^{4} =& activation(z^{4})\\end{align}$$\n",
    "\n",
    "The final results $a^{4}$ are the outputs $a^{4} = \\hat{y}$.\n",
    "\n",
    "Just like in logistic regression, we can compute the derivatives for the final layer, $\\frac{dJ}{dW^{4}}$ and $\\frac{dJ}{da^3}$\n",
    "\n",
    "For the third layer, we compute its own derivatives, $\\frac{da^{3}}{dW^{3}}$ and $\\frac{da^{3}}{da^2}$. Together with the $\\frac{dJ}{da^3}$ received as inputs from the above layer, we have:\n",
    "\n",
    "$$\\frac{dJ}{dW^{3}} = \\frac{dJ}{da^3} \\cdot \\frac{da^{3}}{dW^{3}}$$ \n",
    "and \n",
    "$$\\frac{dJ}{da^{2}} = \\frac{dJ}{da^3} \\cdot \\frac{da^{3}}{da^{2}}$$ \n",
    "\n",
    "For the second layer, we compute its own derivatives, $\\frac{da^{2}}{dW^{2}}$ and $\\frac{da^{2}}{da^1}$. Together with the $\\frac{dJ}{da^2}$ received as inputs from the above layer, we have:\n",
    "\n",
    "$$\\frac{dJ}{dW^{2}} = \\frac{dJ}{da^2} \\cdot \\frac{da^{2}}{dW^{2}}$$ \n",
    "and \n",
    "$$\\frac{dJ}{da^{1}} = \\frac{dJ}{da^1} \\cdot \\frac{da^{2}}{da^{1}}$$ \n",
    "\n",
    "Finally, the input layer computes its own derivatives, $\\frac{da^{1}}{dW^{1}}$, aand together with the $\\frac{dJ}{da^{1}}$ received from the layer above, computes:\n",
    "$$\\frac{dJ}{dW^{1}} = \\frac{dJ}{da^1} \\cdot \\frac{da^{1}}{dW^{1}}$$\n",
    "\n",
    "In backpropagation, **each layer (each operation, really) does the following**:\n",
    "\n",
    "1. Has some inputs I, parameters P, and outputs O.\n",
    "2. Knows show to compute its own derivatives $\\frac{dO}{dP}$ and $\\frac{dO}{dI}$\n",
    "3. Receives as input from the next layer the quantity $\\frac{dJ}{dO}$\n",
    "4. Computes $\\frac{dJ}{dP} = \\frac{dJ}{dO} \\cdot \\frac{dO}{dP}$. This will be used in Gradient Descent.\n",
    "5. Computes $\\frac{dJ}{dI} = \\frac{dJ}{dI} \\cdot \\frac{dO}{dI}$ and passes them back to the preceding layer.\n",
    "\n",
    "Backpropagation is a **computational graph** (sequence of operations) not unlike the model itself is just a sequence of operations.\n",
    "The only difference is that the data travels **backwards**,  from the network output towards its input. The \"data\" here is the gradients (derivatives).\n",
    "\n",
    "Training the model means repeating the two passes:\n",
    "\n",
    "1. **Forward pass**: run the model (from the inputs, and current parameters, compute the outputs and the cost function)\n",
    "2. **Backward pass**: backpropagation + gradient descent (from the cost function, compute gradients and update parameters, going backwards to the input\n",
    "3. Repeat\n",
    "\n",
    "After the gradients are calculated, we can update the parameters.\n",
    "\n",
    "**Gradient descent** refers to the typical update rule $W = W - \\mu \\frac{dJ}{dW}$. There exist also some smarter variations of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matlab functions for working with neural networks\n",
    "\n",
    "- `nprtool()` (for classification)\n",
    "- `nftool()` (for regression)\n",
    "- `nnstart()` or `nntool()`: entry-point for both of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practical work\n",
    "\n",
    "## 3.1 Toy Example (walkthrough)\n",
    "\n",
    "Let's do a sample classification.\n",
    "\n",
    "We shall work with the same `wine_dataset` data as in the previous labs. The last column gives the quality score. We shall treat the quality score as a **class indicator**. We shall perform classification with a multilayer network model, aiming to classify correctly the quality of a wine based on its parameters.\n",
    "\n",
    "Load the `wine_dataset` in Matlab, and consider the following:\n",
    "\n",
    "- how many inputs are there?\n",
    "- how many output categories?\n",
    "- how does the architecture of a 2-layer network for classifying this dataset look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = readmatrix('winequality-red.csv');\n",
    "X = Data(:,1:11);       % 11 columns for the inputs\n",
    "N = size(Data,1);       % The number of wines in the set (1599)\n",
    "\n",
    "Y = Data(:,12) ;        % 12th column the quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do **one hot encoding** of the output vector Y. In R2020b we can use the function `onehotencode()`. \n",
    "In previous versions, we can use the function `ind2vec()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yenc = full(ind2vec(Y'))\n",
    "% Since our data starts from value 3, we shall remove the first two lines. Thus quality 3 = class 1, quality 4 = class 2, etc.\n",
    "Yenc = Yenc(3:end, :)' % Remove first two lines\n",
    "Yenc = Yenc';          % Transpose, so each value corresponds to a row, just like the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the `nprtool()` or the `nnstart()`.\n",
    "\n",
    "1. Select the input and output data.\n",
    "\n",
    "2. Select the size of the **training, cross-validation and testing sets**.\n",
    "\n",
    " - The **training set** contains the data used for the actual training.\n",
    " - The **validation set** contains the data used to stop the training before over-fitting (over-learning). Listen to the explanations, it's too long to write here. For a fair decision, for validation we use data which is not used for actual training.\n",
    " - The **testing set** contains the data used for the final testing of the final trained model. For a fair result, we use data which the network has never seen until now (neither in training nor for validation).\n",
    " \n",
    "  Default values for small datasets are around 70% / 15% / 15%. For larger datasets (tens of thousands of input vectors), we can use smaller percents for validation and testing.\n",
    "\n",
    "  These three sets of data are selected **randomly** from the overall available dataset, for fairness.\n",
    "\n",
    "3. Select the size of the **hidden layer**. This can be any value, but typically one chooses some value between the input size (large) and output size (small), so that the network has an overall decreasing size towards the output.\n",
    "\n",
    "  We can go with the default value (10), or perhaps make it 8 or 9. We can experiment with different values.\n",
    "  \n",
    "  We can put a vector here, e.g. [10 9 8], and we shall have **three hidden layers** instead of just one, therefore a much longer network.\n",
    "\n",
    "4.Train the network (press `Train` button), and when finished, investigate the results:\n",
    "  - Number of epochs and final error value\n",
    "  - Error plot\n",
    "  - Confusion matrix\n",
    "  - Receiver Operating Characteristic\n",
    "\n",
    "5. Go back and change the hidden layer size to 20, 50, then 5. Do the results change significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a network in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = patternnet(10);      % Create a network with one hidden layer of size 10. Rest of the params remain with default values.\n",
    "net = train(net,X',Yenc'); % Train the network with our data. The data should be arranged on rows.\n",
    "\n",
    "view(net)                  % View the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInally, do some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = net([put some input values here])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Serious example: handwritten digit classification on MNIST\n",
    "\n",
    "We follow the MNIST training tutorial from [https://www.mathworks.com/matlabcentral/fileexchange/73010-mnist-neural-network-training-and-testing](https://www.mathworks.com/matlabcentral/fileexchange/73010-mnist-neural-network-training-and-testing)\n",
    "\n",
    "Unzip the MNIST data. \n",
    "\n",
    "Load the data (this will create the matrix `mnist_train`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load mnist_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the digit number. The remaining 748 columns contain the pixels in the linearized image (every image has 28x28 pixels).\n",
    "Lookup the MNIST database on the Internet to see how the images look like.\n",
    "\n",
    "Let's prepare the inputs and outputs, like we did before. Check the sizes of the resulting arrays to understand what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_train(:,2:end);\n",
    "Y = mnist_train(:,1);\n",
    "Yenc = full(ind2vec(1 + Y'))   % digits are 0 to 9, add 1 so that class indices are 1 to 10 (10 classes)\n",
    "Yenc = Yenc';          % Transpose, so each value corresponds to a row, just like the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a shallow 2-layer network to classifiy the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygreatnetwork = patternnet([80, 60]);            % Use two hidden layers, with size 80 and 60\n",
    "mygreatnetwork = train(mygreatnetwork, X',Yenc);  % Train the network with our data. The data should be arranged properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a real-life problem is not very fast :) Have patience until training is finished.\n",
    "\n",
    "You can look a the performance and confusion matrix updating live as training progresses.\n",
    "\n",
    "When training is finished:\n",
    "  - Look at the error plot. Notice **overfitting**? This is what validation is good for.\n",
    "  - Look at the confusion matrix. What is the classification accuracy (percentage of correctly classified images) on the testing set? \n",
    "**Around 96%**. Excellent results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to predict the digit from your own hand-drawing. \n",
    "\n",
    "Draw a digit similar to the ones in the dataset (this amounts to the **preprocessing** of the image):\n",
    "  - Draw a letter using the mouse, in MS Paint\n",
    "  - Make sure the digit is around the center of the image, has the same brush width compared to the image as the ones in the dataset\n",
    "  - Save it as a grayscale image\n",
    "  - Resize the image to 28x28 pixels\n",
    "  - You should get an image similar to the ones in the dataset\n",
    "  \n",
    "Read the image in Matlab:\n",
    "  - Use `imread()` to load the image\n",
    "  - Convert to double with (`double()`)\n",
    "  - Linearize the matrix in a row-major order (row by row), e.g.: `Ivec = I'(:)`\n",
    "  \n",
    "Identify (predict) the digit using the network:\n",
    "```\n",
    "mygreatnetwork(Ivec)\n",
    "```\n",
    "\n",
    "Was the result correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters\n",
    "\n",
    "Change the network architecture: use a single hidden layer, change the number of neurons, etc. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final questions\n",
    "\n",
    "1. How many parameters does the network used for MNIST classification have? \n",
    "\n",
    "2. How many parameters would have a network used for classifying color images with 1024 x 768 resolution, into 10 output classes, using two hidden layers of size 2000 and 150?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
