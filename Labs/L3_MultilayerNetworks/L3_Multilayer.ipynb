{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Laboratory 03: MultiLayer Perceptron networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "Students should understand and be able use a multi-layer fully-connected networks in Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical aspects\n",
    "\n",
    "Multi-layer perceptron (fully-connected) neural networks are widely used for classificaton of small, simple datasets.\n",
    "\n",
    "### Multilayer perceptron\n",
    "\n",
    "#### Cascading neurons \n",
    "\n",
    "We start from last week's essential message: **A single neuron creates a single hyperplane and separates the input space in two categories 0 or 1)**\n",
    "  - **\"neuron\"** = one logistic regression operation\n",
    "  - **\"hyperplane\"** = a linear boundary surface, with dimension N-1 \n",
    "  - with a smooth sigmoid transition zone between the two classes\n",
    "  \n",
    "What if we have a dataset as follows? How to do classification here?\n",
    "\n",
    "<div>\n",
    "<img src=img/DatasetAngled1.png align=\"center\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "![How to separate the classes in this dataset? ]()\n",
    "\n",
    "\n",
    "Solution: use **two neurons**: \n",
    "  \n",
    "  - each one draws a hyperplane (a line)\n",
    "  - aggregate their results into the final outcome: \"When both neurons say 1, output class is 1. Otherwise, output class is 0\".\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=img/DatasetAngled2.png align=\"center\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Combining the results of both neurons in the final result is **also done with a (third) neuron**. Thus, we have **cascading neurons**.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=img/Network2plus1.png align=\"center\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Neurons operating on the same inputs form a **layer**. We have two layers now:\n",
    "\n",
    "- The inputs (this does not contain neurons, just the inputs, but it is commonly named \"the input layer\")\n",
    "- The hidden layer (middle)\n",
    "- The output layer (the output neuron)\n",
    "\n",
    "What if we want a boundary composed of 3 sides? Use three neurons in the hidden layer.\n",
    "\n",
    "What if we want a curved boundary? Use many more neurons (approximate the curve from many lines)\n",
    "\n",
    "**Any hypersurface** can be obtained with just two layers, provided there are enough neurons in the hidden layer:\n",
    "\n",
    "  1. The hidden layer draws some hyperplanes (e.g. lines)\n",
    "  2. The output layer combines the results into output values\n",
    "\n",
    "#### Multiple outputs\n",
    "\n",
    "What if we have 4 output classes?\n",
    "\n",
    "Have 4 neurons in the output layer, one for each class. When the input belongs to class $k$, the $k$-th neuron should produce 1, and all the others should produce 0.\n",
    "\n",
    "**One-hot encoding**: When we train the network, we need to tell it what is the desired output (target). This is known as **encoding**. \n",
    "For an input of class $k$, we tell the network to produce a vector with a single value of 1, on position $k$.\n",
    "$$\\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{bmatrix}$$\n",
    "\n",
    "After training, when running the model, we look at **the location** of the highest value and the location is the predicted class.\n",
    "\n",
    "#### Multiple layers\n",
    "\n",
    "We can actually have more than 2 layers in a network. We can have as many as we want! Interpretation:\n",
    "\n",
    "   - first hidden layer draws some hyperplanes\n",
    "   - next layer combines hyperplanes into some simpler shapes\n",
    "   - next layer combines the simple shapes into more complex shapes\n",
    "   - ....\n",
    "   - final layer gives the output\n",
    "\n",
    "In practice, it is often better to have **more layers with fewer neurons** than 2 layers but with a huge hidden layer.\n",
    "\n",
    "However, training many layers and many neurons is **difficult**, i.e. it can overfit, become unstable, etc.\n",
    "\n",
    "#### Matrix form\n",
    "\n",
    "**One neuron** does a linear combination of the inputs, followed by activation function:\n",
    "$$\\begin{bmatrix} w_1 & x_w & \\dots & w_N & b \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1 \\end{bmatrix} = z \n",
    "\\rightarrow a$$\n",
    "\n",
    "**A layer of $M$ neurons is just $M$ neurons next to each other**:\n",
    "$$\\begin{bmatrix} \n",
    "w_{11} & w_{12} & \\dots & w_{1N} & b_1 \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2N} & b_2 \\\\\n",
    "\\vdots  & \\vdots & \\dots & \\vdots & \\vdots \\\\\n",
    "w_{M1} & w_{M2} & \\dots & w_{MN} & b_M \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "z_1 \\\\ z_2 \\\\ \\dots \\\\ z_M\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \n",
    "\\begin{bmatrix} \n",
    "a_1 \\\\ a_2 \\\\ \\dots \\\\ a_M\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each layer is characterized by the weight matrix $W$.\n",
    "\n",
    "The whole network can be understood as a sequence of matrix multiplications and activation functions.\n",
    "\n",
    "**The next layer** takes as inputs the outputs $a_i$ of the previos layer, and does the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The multi-layer perceptron model contains $L$ layers, each layer consisting of a matrix multiplication and activation function:\n",
    "\n",
    "$$\\begin{align}z^{1} =& W^{1} \\cdot X \\\\\n",
    "a^{1} =& activation(z^{1}) \\\\\n",
    "\\\\\n",
    "z^{2} =& W^{2} \\cdot a^{1} \\\\\n",
    "a^{2} =& activation(z^{2}) \\\\\n",
    "\\\\ \n",
    "... \\\\\n",
    "z^{k} =& W^{k} \\cdot a^{k-1} \\\\\n",
    "a^{k} =& activation(z^{k})\\end{align}$$\n",
    "\n",
    "Here, $W^{k}$ is a matrix and $z{k}$, $a{k}$ are vectors (columns).\n",
    "\n",
    "The activation function can be the **sigmoid**, **ReLU**, **tanh** etc. Typically the output used sigmoid, but all others are up to the designer.\n",
    "\n",
    "**Inputs**: \n",
    "\n",
    "   - a matrix $X$ with every input vector being a column (according to the equations below; we can also transpose all matrices and vectors, \n",
    "   if we want).\n",
    " \n",
    "**Outputs** (assuming one-hot encoding):\n",
    " \n",
    "  - a vector $\\hat{y}$ which should be understood as scores/probability of belonging in each class\n",
    "  - the **location of the maximum** value gives the predicted class\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model parameters\n",
    "\n",
    "The model parameters are the weight matrices $W^{k}$ of every layer. The element $w^{k}_{ij}$ is the weight in the $k$-th layer, $i$-th neuron, $j$-th input of it.\n",
    "\n",
    "Every neuron has a bias input. We presume that the bias is included in the weight matrices,\n",
    "like we did until now (e.g. like a fake input equal to 1 is appended to the input of every layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost function\n",
    "\n",
    "For classification, the cross-entropy is used.\n",
    "\n",
    "For a single input:\n",
    "\n",
    "$$L(y, \\hat{y}) = - y_1 \\log{\\hat{y_1}} - \\dots - y_n \\log{\\hat{y_n}} = -\\log{\\hat{y_{class}}},$$\n",
    "\n",
    "where $\\hat{y_{class}}$ is the model's predicted probability for the true class of the input.\n",
    "\n",
    "For multiple inputs: do the average of all\n",
    "$$J = \\frac{1}{N} \\sum_i L(y^i, \\hat{y}^i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is done with **backpropagation** and gradient descent (or some variant of it).\n",
    "\n",
    "**Backpropagation** = the technique to compute the derivatives of $J$  with respect to all parameters in the network.\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "Assume we have a network with 4 layers.\n",
    "\n",
    "$$\\begin{align}z^{1} =& W^{1} \\cdot X \\\\\n",
    "a^{1} =& activation(z^{1}) \\\\\n",
    "\\\\\n",
    "z^{2} =& W^{2} \\cdot a^{1} \\\\\n",
    "a^{2} =& activation(z^{2}) \\\\\n",
    "\\\\ \n",
    "z^{3} =& W^{3} \\cdot a^{2} \\\\\n",
    "a^{3} =& activation(z^{3}) \\\\\n",
    "\\\\ \n",
    "z^{4} =& W^{4} \\cdot a^{3} \\\\\n",
    "a^{4} =& activation(z^{4})\\end{align}$$\n",
    "\n",
    "The final results $a^{4}$ are the outputs $a^{4} = \\hat{y}$.\n",
    "\n",
    "Just like in logistic regression, we can compute the derivatives for the final layer, $\\frac{dJ}{dW^{4}}$ and $\\frac{dJ}{da^3}$\n",
    "\n",
    "For the third layer, we compute its own derivatives, $\\frac{da^{3}}{dW^{3}}$ and $\\frac{da^{3}}{da^2}$. Together with the $\\frac{dJ}{da^3}$ received as inputs from the above layer, we have:\n",
    "\n",
    "$$\\frac{dJ}{dW^{3}} = \\frac{dJ}{da^3} \\cdot \\frac{da^{3}}{dW^{3}}$$ \n",
    "and \n",
    "$$\\frac{dJ}{da^{2}} = \\frac{dJ}{da^3} \\cdot \\frac{da^{3}}{da^{2}}$$ \n",
    "\n",
    "For the second layer, we compute its own derivatives, $\\frac{da^{2}}{dW^{2}}$ and $\\frac{da^{2}}{da^1}$. Together with the $\\frac{dJ}{da^2}$ received as inputs from the above layer, we have:\n",
    "\n",
    "$$\\frac{dJ}{dW^{2}} = \\frac{dJ}{da^2} \\cdot \\frac{da^{2}}{dW^{2}}$$ \n",
    "and \n",
    "$$\\frac{dJ}{da^{1}} = \\frac{dJ}{da^1} \\cdot \\frac{da^{2}}{da^{1}}$$ \n",
    "\n",
    "Finally, the input layer computes its own derivatives, $\\frac{da^{1}}{dW^{1}}$, aand together with the $\\frac{dJ}{da^{1}}$ received from the layer above, computes:\n",
    "$$\\frac{dJ}{dW^{1}} = \\frac{dJ}{da^1} \\cdot \\frac{da^{1}}{dW^{1}}$$\n",
    "\n",
    "In backpropagation, **each layer (each operation, really) does the following**:\n",
    "\n",
    "1. Has some inputs I, parameters P, and outputs O.\n",
    "2. Knows show to compute its own derivatives $\\frac{dO}{dP}$ and $\\frac{dO}{dI}$\n",
    "3. Receives as input from the next layer the quantity $\\frac{dJ}{dO}$\n",
    "4. Computes $\\frac{dJ}{dP} = \\frac{dJ}{dO} \\cdot \\frac{dO}{dP}$. This will be used in Gradient Descent.\n",
    "5. Computes $\\frac{dJ}{dI} = \\frac{dJ}{dI} \\cdot \\frac{dO}{dI}$ and passes them back to the preceding layer.\n",
    "\n",
    "Backpropagation is a **computational graph** (sequence of operations) not unlike the model itself is just a sequence of operations.\n",
    "The only difference is that the data travels **backwards**,  from the network output towards its input. The \"data\" here is the gradients (derivatives).\n",
    "\n",
    "Training the model means repeating the two passes:\n",
    "\n",
    "1. **Forward pass**: run the model (from the inputs, and current parameters, compute the outputs and the cost function)\n",
    "2. **Backward pass**: backpropagation + gradient descent (from the cost function, compute gradients and update parameters, going backwards to the input\n",
    "3. Repeat\n",
    "\n",
    "After the gradients are calculated, we can update the parameters.\n",
    "\n",
    "**Gradient descent** refers to the typical update rule $W = W - \\mu \\frac{dJ}{dW}$. There exist also some smarter variations of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matlab functions for working with neural networks\n",
    "\n",
    "- `nprtool()` (for classification)\n",
    "- `nftool()` (for regression)\n",
    "- `nnstart()` or `nntool()`: entry-point for both of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (walkthrough)\n",
    "\n",
    "Let's do a sample classification.\n",
    "\n",
    "Load the `wine_dataset` in Matlab and inspect the data:\n",
    "\n",
    "- how many inputs are there?\n",
    "- how many output categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load wine_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the `nprtool()` or the `nnstart()` and run training.\n",
    "\n",
    "FInally, do some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practical work\n",
    "\n",
    "We use the same data as in linear regression, but instead we try to predict if one of the two possibilities: the quality score is <=5 (class 0) or the quality score is > 5 (class 1).\n",
    "\n",
    "As a reminder, the data comes from here: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009, and contains 11 numerical chemical measurements for some different brands of red wines, together with a quality score indicated by buyers (quality goes from 3 to 8).\n",
    "\n",
    "Inputs:\n",
    "\n",
    "   - 1 - fixed acidity\n",
    "   - 2 - volatile acidity\n",
    "   - 3 - citric acid\n",
    "   - 4 - residual sugar\n",
    "   - 5 - chlorides\n",
    "   - 6 - free sulfur dioxide\n",
    "   - 7 - total sulfur dioxide\n",
    "   - 8 - density\n",
    "   - 9 - pH\n",
    "   - 10 - sulphates\n",
    "   - 11 - alcohol \n",
    "   \n",
    "Outputs:\n",
    "\n",
    "   - 12 - quality\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Data = readmatrix('winequality-red.csv');\n",
    "X = Data(:,1:11);       % 11 columns for the inputs\n",
    "N = size(Data,1);       % The number of wines in the set (1599)\n",
    "\n",
    "Y = Data(:,12) > 5;     % make 1 column for the output: 1 if score > 5, 0 if score <= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the X matrix so we can treat the bias $b$ as just another weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = [X ones(N,1)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the weights to some random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W =\n",
      "\n",
      "    2.7694\n",
      "   -1.3499\n",
      "    3.0349\n",
      "    0.7254\n",
      "   -0.0631\n",
      "    0.7147\n",
      "   -0.2050\n",
      "   -0.1241\n",
      "    1.4897\n",
      "    1.4090\n",
      "    1.4172\n",
      "    0.6715\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = randn(12, 1)   % a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Compute and show the cost function with the above weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Implement optimization with Gradient Descent\n",
    "\n",
    "You can implement a visualizaiton just like in the example provided, by copying and adapting the code.\n",
    "You cannot plot all the 11 dimensions of the input data, so pick only two of them to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% To fill in\n",
    "%======================\n",
    "\n",
    "W = randn(12, 1);           % initialize parameters randomly\n",
    "\n",
    "number_of_epochs = 1000;    % set number of iterations\n",
    "\n",
    "for iter = 1:number_of_epochs\n",
    "    \n",
    "    % Compute predictions:\n",
    "    Y-pred = ...\n",
    "    \n",
    "    % Compute cost:\n",
    "    J(iter) = 1/N * ...\n",
    "    \n",
    "    % Compute derivatives according to the given formula\n",
    "    dW = ...\n",
    "    \n",
    "    % Update the weights\n",
    "    mu = 0.0001;           % try multiple values here\n",
    "    W = W - mu * dW;\n",
    "    \n",
    "    % Store the weights history\n",
    "    W_hist(:,i) = W;\n",
    "end\n",
    "\n",
    "% Plot the error and the evolution of the weights\n",
    "plot(J)\n",
    "figure\n",
    "plot(W_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Compute the solution with the Matlab function `fitglm()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final questions\n",
    "\n",
    "1. In our example, the parameters $W$ keep updating forever, making the gray transition area smaller and smaller, but the actual frontier does not change much. Why does this happen? How can we prevent it?\n",
    "\n",
    "2. What happens if the two classes are **unbalanced** (many more inputs in one class compared to the other)?\n",
    "\n",
    "2. Suggest some good termination conditions for Gradient Descent (i.e. when should we stop the iterations)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
