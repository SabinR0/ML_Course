{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Laboratory 04: Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "Students should understand and be able use some basic pretrained CNN models available in Matlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical aspects\n",
    "\n",
    "CNNs are multilayer networks adapted for image processing, which use the **convolution** operation extensively.\n",
    "\n",
    "### Starting points\n",
    "1. The layers in a normal multilayer perceptron network (MLP) are **fully-connected**: each output value is a combination of all inputs \n",
    "2. Each full-connected layer is a **full (dense) matrix**\n",
    "\n",
    "The number of parameters in fully-connected layers is **huge**. \n",
    "\n",
    "**Example** Consider a layer with input size =  300 x 300 color pixels, and output size equivalent to 150 x 150 color pixels. How many parameters does this layer have?\n",
    "\n",
    "1. Images are large: 1 Mexapixel color image = 3 million values\n",
    "2. Fully-connected layers have huge size\n",
    "\n",
    "### Convolution\n",
    "\n",
    "DSP deja-vu vibes: $$y[n] = \\sum_k x[n-k] h[k]$$\n",
    "\n",
    "- Some videos here: a **kernel** with fixed coefficients $h[k]$ is slided over the input $x[n]$ and computes the output as a linear combination of the surrounding input samples\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Conolution has been used for ever in signal and image processing for **extracting features** (edges, frequency bands, etc)\n",
    "\n",
    "- Convolution is a kind of matrix multiplication, with an almost sparse matrix of a special form (\"circulant\" or \"Toeplitz\")\n",
    "\n",
    "- Each output value depends only on the surrounding pixels\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "- The basic unit is now **a layer**\n",
    "\n",
    "- The data is viewed as **tensors**: 3D cubes of data (like a three-dimensional matrix)\n",
    "\n",
    "- Each layer takes as input an $M_1 \\times N_1 \\times C_1$ tensor and produces an output $M_2 \\times N_2 \\times C_2$ tensor\n",
    "\n",
    "- We don't think of individual neurons anymore. Each neuron in a convolutional layer does exactly the same operation as the others, with the same weights, but \"sees\" just one small part of the input image\n",
    "\n",
    "### Architecture of a CNN\n",
    "\n",
    "**AlexNet:**\n",
    "\n",
    "[1] Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017-05-24). \"ImageNet classification with deep convolutional neural networks\" (PDF). Communications of the ACM. 60 (6): 84â€“90. doi:10.1145/3065386. ISSN 0001-0782.\n",
    "\n",
    "![AlexNet architecture (image from (https://www.mdpi.com/2072-4292/9/8/848/htm)](img/AlexNet.webp)\n",
    "\n",
    "\n",
    "#### Layer types\n",
    "\n",
    "Open the AlexNet model in Matlab and look at the architecture directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = alexnet('Weights','imagenet')\n",
    "analyzeNetwork(net)  % or double-click `net` in the Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer types:\n",
    "\n",
    "- Convolutional\n",
    "- Activation\n",
    "- Pooling (MaxPooling)\n",
    "- Fully connected (e.g. like in multilayer perceptron)\n",
    "- Softmax activation to get probability-like scores (like in multilayer perceptron)\n",
    "- Other optional stuff: normalization, dropout, batch normalization etc\n",
    "\n",
    "A CNN can be viewed as a feature extractor + classificator:\n",
    " \n",
    "- The output layers are very similar to multilayer perceptron: fully-connected layers + softmax\n",
    "- The first part, with the convolution layers, is a kind of **feature extractor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "A CNN has a fixed arcihtecture composed of several layers. For typical networks, the architecture is described in the accompanying papers.\n",
    "\n",
    "**Inputs**: \n",
    "\n",
    "   - a color image represented as a tensor $X$ of size $L_1 \\times L_2 \\times 3$\n",
    " \n",
    "**Outputs** (assuming one-hot encoding):\n",
    " \n",
    "  - a vector $\\hat{y}$ which should be understood as scores/probability of belonging in each class\n",
    "  - the **location of the maximum** value gives the predicted class\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model parameters\n",
    "\n",
    "The model parameters are the parameters of the layers:\n",
    "\n",
    " - the filter coefficients in the convolutional layers\n",
    " \n",
    " - the weights in the fully-connected layers\n",
    " \n",
    " The number of parameters of the convolutional layers is **much smaller** than for fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost function\n",
    "\n",
    "For classification, the cross-entropy is typically used. This is exactly similar to how it is used in MLP networks.\n",
    "\n",
    "For a single input:\n",
    "\n",
    "$$L(y, \\hat{y}) = - y_1 \\log{\\hat{y_1}} - \\dots - y_n \\log{\\hat{y_n}} = -\\log{\\hat{y_{class}}},$$\n",
    "\n",
    "where $\\hat{y_{class}}$ is the model's predicted probability for the true class of the input.\n",
    "\n",
    "For multiple inputs: do the average of all\n",
    "$$J = \\frac{1}{N} \\sum_i L(y^i, \\hat{y}^i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is done with **backpropagation** and gradient descent (or some variant of it).\n",
    "\n",
    "**Backpropagation** = the technique to compute the derivatives of $J$  with respect to all parameters in the network.\n",
    "\n",
    "Same story as for multilayer perceptron (MLP) networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matlab functions for working with CNNs\n",
    "\n",
    "Lots of new functions, there is a massive push in this direction in the last 2-3 years. Search the docs, there are many nice tutorials.\n",
    "\n",
    "### Other frameworks besides Matlab\n",
    "\n",
    "The most used deep learning frameworks are written in Python:\n",
    "\n",
    "- Tersorflow + Keras\n",
    "- Pytorch\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practical work\n",
    "\n",
    "Reference: https://www.mathworks.com/help/deeplearning/ug/transfer-learning-using-alexnet.html\n",
    "\n",
    "1. Load the AlexNet model with pretrained weights on the ImageNet database. Open the model and examine the architecure.\n",
    "\n",
    "  - How many parameters does the first convolutional layer have?\n",
    "  - How many parameters does each trainable layer have? (trainable = convolutional or fully-connected)\n",
    "  - What is the share of the fully-connected layers in the total number of parameters?\n",
    "\n",
    "\n",
    "2. Play with AlexNet. Download an image from the Internet and classify it. Does it work?\n",
    "\n",
    "  The ImageNet class names can be found here: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "  \n",
    "3. Perform **Transfer Learning** with AlexNet following the tutorial in here:  https://www.mathworks.com/help/deeplearning/ug/transfer-learning-using-alexnet.html\n",
    "\n",
    "4. Use a different model:\n",
    "\n",
    "  - Check out the available pretrained CNN models in Matlab: https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html\n",
    "  - Try one of the smaller, but more accurate ones: GoogLeNet, Resnet-18, Mobile-net v2\n",
    "  \n",
    "  \n",
    "5. Compare other networks (GoogLeNet, Resnet-18, Mobile-net v2) with AlexNet:\n",
    "\n",
    "  - do they have more or less layers?\n",
    "  - do they have smaller or bigger fully-connected layers a the end?\n",
    "  \n",
    "\n",
    "6. Add one extra convolutional layer in the middle part of AlexNet. Train the network (warning: it may take a long long time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final questions\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
